{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75093812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCtIoR-hOQZTzaYg9YcDHhhzONsLFFstYQ\"\n",
    "\n",
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "response = completion(\n",
    "    model=\"gemini/gemini-2.5-pro\", \n",
    "    messages=[{\"role\": \"user\", \"content\": \"write code for saying hi from LiteLLM\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f36a7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course! Here\\'s the code to \"say hi\" using LiteLLM, along with a clear explanation of each step.\\n\\nLiteLLM acts as a universal translator for over 100 Large Language Model (LLM) APIs. The code below will send a prompt (\"Say hi\") to an AI model (we\\'ll use OpenAI\\'s `gpt-3.5-turbo` as a default example) and then print the model\\'s friendly response.\\n\\n### Prerequisites\\n\\n1.  **Python:** Make sure you have Python installed on your system.\\n2.  **API Key:** LiteLLM is a wrapper, not a free model provider. You need an API key from an LLM provider like [OpenAI](https://platform.openai.com/api-keys), [Groq](https://console.groq.com/keys), or [Anthropic](https://console.anthropic.com/dashboard). For this example, we\\'ll assume you have an OpenAI API key.\\n\\n---\\n\\n### Step 1: Install LiteLLM\\n\\nFirst, open your terminal or command prompt and install the necessary libraries.\\n\\n```bash\\npip install litellm openai\\n```\\n*(We install `openai` because it\\'s a common dependency that LiteLLM uses behind the scenes for OpenAI models).*\\n\\n---\\n\\n### Step 2: The Python Code\\n\\nCreate a new Python file (e.g., `say_hi.py`) and paste the following code into it.\\n\\nThis code is written using best practices, like using environment variables for your API key to keep it secure.\\n\\n```python\\nimport litellm\\nimport os\\nimport sys\\n\\n# --- Configuration ---\\n# For security, it\\'s best to set your API key as an environment variable.\\n# LiteLLM automatically finds keys for major providers like OpenAI, Anthropic, etc.\\n#\\n# To set it in your terminal:\\n# for Mac/Linux: export OPENAI_API_KEY=\"your-api-key-here\"\\n# for Windows:   set OPENAI_API_KEY=\"your-api-key-here\"\\n#\\n# If you don\\'t set it, the code will prompt you to enter it.\\nif \"OPENAI_API_KEY\" not in os.environ:\\n    try:\\n        api_key = input(\"Please enter your OpenAI API key: \")\\n        os.environ[\"OPENAI_API_KEY\"] = api_key\\n    except EOFError:\\n        print(\"\\\\nCould not read API key. Please set the OPENAI_API_KEY environment variable.\")\\n        sys.exit(1)\\n\\n\\n# --- The Core Logic ---\\n\\ndef say_hi_with_litellm():\\n    \"\"\"\\n    Uses LiteLLM to call an AI model and get a greeting.\\n    \"\"\"\\n    print(\"Sending a request to the AI model via LiteLLM...\")\\n\\n    try:\\n        # 1. Define the messages for the model\\n        #    This follows the standard chat completion format.\\n        messages = [\\n            {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\\n            {\"role\": \"user\", \"content\": \"Say hi in a creative and friendly way.\"}\\n        ]\\n\\n        # 2. Make the call using litellm.completion()\\n        #    This is the main function. You just need to specify the model\\n        #    and the messages. LiteLLM handles the rest.\\n        response = litellm.completion(\\n            model=\"gpt-3.5-turbo\",  # Using a standard OpenAI model\\n            messages=messages\\n        )\\n\\n        # 3. Extract the message content from the response\\n        #    The response object has a standard structure.\\n        ai_message = response.choices[0].message.content\\n\\n        # 4. Print the greeting!\\n        print(\"\\\\n\" + \"=\"*30)\\n        print(\"The AI says:\")\\n        print(f\"-> {ai_message}\")\\n        print(\"=\"*30)\\n\\n    except Exception as e:\\n        # Handle potential errors, like invalid API keys or network issues\\n        print(f\"\\\\nAn error occurred: {e}\")\\n        print(\"Please check your API key and network connection.\")\\n\\n# --- Run the function ---\\nif __name__ == \"__main__\":\\n    say_hi_with_litellm()\\n\\n```\\n\\n---\\n\\n### Step 3: How to Run the Code\\n\\n1.  **Set your API Key (Recommended):**\\n    *   **On macOS or Linux:**\\n        ```bash\\n        export OPENAI_API_KEY=\"sk-xxxxxxxxxxxxxxxxxxxxxxxx\"\\n        ```\\n    *   **On Windows (Command Prompt):**\\n        ```bash\\n        set OPENAI_API_KEY=\"sk-xxxxxxxxxxxxxxxxxxxxxxxx\"\\n        ```\\n    *Replace `sk-xxxxxxxxxxxxxxxxxxxxxxxx` with your actual OpenAI API key.*\\n\\n2.  **Run the Python script:**\\n    From the same terminal window, run the file.\\n    ```bash\\n    python say_hi.py\\n    ```\\n\\n### Expected Output\\n\\nYou will see something like this in your terminal:\\n\\n```\\nSending a request to the AI model via LiteLLM...\\n\\n==============================\\nThe AI says:\\n-> Hello there! It\\'s wonderful to connect with you. Hope you\\'re having a fantastic day! ðŸ‘‹\\n==============================\\n```\\n\\n---\\n\\n### Bonus: Saying Hi with a Different Model (e.g., Groq)\\n\\nThe real power of LiteLLM is how easy it is to switch models. Let\\'s say you want to use a super-fast, free model from **Groq**.\\n\\n1.  Get a free API key from [Groq](https://console.groq.com/keys).\\n2.  Set the new environment variable: `export GROQ_API_KEY=\"your-groq-key\"`\\n3.  **Change only one line** in your Python code:\\n\\n    ```python\\n    # Change this line:\\n    # model=\"gpt-3.5-turbo\",\\n\\n    # To this:\\n    model=\"groq/llama3-8b-8192\",\\n    ```\\n\\nNow, when you run the script, LiteLLM will automatically use your Groq key and call the Llama 3 model on Groq\\'s infrastructure, all without changing any other code'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bearllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
